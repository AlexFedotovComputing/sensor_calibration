{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Температурный пайплайн (модульный)\n*Последнее обновление: 2025-09-07*\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# === 1) Параметры минимального пайплайна ===\nimport os, io, csv, glob\nfrom typing import List, Tuple, Optional, Dict\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display\n\n# Параметры\nDATE_FORMAT = ''\nREF_IDX = 8\nN_FOLLOW = 3\nWINDOW_N = 20\nSTD_THR = 5e-2\nDIFF_THR = 2e-3\nMIN_LEN = 20\nDEG_TOL = 1.0\nMAX_REF_DRIFT = 0.3\nGROUP_BY_FILE = True\n\n# Имена датчиков для отчёта\nREF_NAME = globals().get('REF_NAME', 'Эталон T8')\nSENSOR_NAMES = globals().get('SENSOR_NAMES', {\n    'T9': 'Датчик 1',\n    'T10': 'Датчик 2',\n    'T11': 'Датчик 3',\n})\n# Альтернатива: список имён в порядке датчиков T{REF_IDX+1}..T{REF_IDX+N_FOLLOW}\nFOLLOWER_NAMES = globals().get('FOLLOWER_NAMES', ['Датчик 1','Датчик 2','Датчик 3'])\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# === 2) Загрузка/парсинг ===\ndef sniff_sep(sample: bytes) -> str:\n    try:\n        dialect = csv.Sniffer().sniff(sample.decode('utf-8', errors='ignore'),\n                                      delimiters=[',',';','\\t','|'])\n        return dialect.delimiter\n    except Exception:\n        line = sample.decode('utf-8', errors='ignore').splitlines()[0] if sample else ''\n        for cand in [',',';','\\t','|']:\n            if line.count(cand) >= 1:\n                return cand\n        return ','\n\ndef read_one_table(name: str, stream: io.BytesIO, date_format: Optional[str]=None) -> pd.DataFrame:\n    head = stream.read(8192); stream.seek(0)\n    sep = sniff_sep(head)\n    df = pd.read_csv(stream, sep=sep, engine='python')\n    if df.shape[1] < 17:\n        raise ValueError(f\"{name}: найдено {df.shape[1]} столбцов, требуется >= 17 (1 дата + 16 температур).\")\n    df = df.iloc[:, :17].copy()\n    df.columns = ['date'] + [f'T{i}' for i in range(16)]\n    if date_format and date_format.strip():\n        df['date'] = pd.to_datetime(df['date'], format=date_format, errors='coerce')\n    else:\n        df['date'] = pd.to_datetime(df['date'], infer_datetime_format=True, errors='coerce', dayfirst=True)\n    if df['date'].isna().any():\n        bad = int(df['date'].isna().sum()); print(f\"[Предупреждение] {name}: {bad} строк с нераспознанной датой отброшены.\")\n        df = df.dropna(subset=['date'])\n    for c in [f'T{i}' for i in range(16)]:\n        df[c] = pd.to_numeric(df[c], errors='coerce')\n    df['source_file'] = name\n    return df\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# === 3) Функции стабильности (минимум) ===\nimport numpy as np\nimport pandas as pd\nfrom typing import List, Tuple, Sequence, Dict, Optional\n\ndef rolling_std_mask(series, window, threshold):\n    rs = series.rolling(window=window, min_periods=window).std()\n    return (rs <= threshold)\n\ndef rolling_mean_abs_diff_mask(series, window, diff_threshold):\n    d = series.diff().abs()\n    m = d.rolling(window=window, min_periods=window).mean()\n    return (m <= diff_threshold)\n\ndef segments_from_mask(mask, window):\n    import numpy as _np\n    arr = mask.to_numpy() if hasattr(mask, 'to_numpy') else _np.asarray(mask)\n    segs = []; cur=None\n    for i, ok in enumerate(arr):\n        if ok:\n            s = max(0, i - window + 1); e = i\n            if cur is None: cur=(s,e)\n            else:\n                cs, ce = cur\n                if s <= ce + 1: cur=(cs, max(ce, e))\n                else: segs.append(cur); cur=(s,e)\n        else:\n            if cur is not None: segs.append(cur); cur=None\n    if cur is not None: segs.append(cur)\n    return segs\n\ndef summarize_interval(df, cols, s, e):\n    row={'start_idx':int(s),'end_idx':int(e),'length':int(e-s+1)}\n    row['start_date']=pd.to_datetime(df.loc[s,'date']) if s < len(df) else pd.NaT\n    row['end_date']=pd.to_datetime(df.loc[e,'date']) if e < len(df) else pd.NaT\n    for c in cols:\n        vals = df[c].to_numpy()[s:e+1]\n        good = ~np.isnan(vals)\n        row[f'mean_{c}'] = float(np.nanmean(vals)) if good.any() else np.nan\n        row[f'std_{c}'] = float(np.nanstd(vals, ddof=1)) if good.sum()>1 else np.nan\n        row[f'min_{c}'] = float(np.nanmin(vals)) if good.any() else np.nan\n        row[f'max_{c}'] = float(np.nanmax(vals)) if good.any() else np.nan\n        row[f'drift_{c}'] = row[f'max_{c}'] - row[f'min_{c}'] if good.any() else np.nan\n    return row\n\ndef median_level(series, s, e):\n    vals = series.to_numpy()[int(s):int(e)+1]\n    return float(np.nanmedian(vals))\n\ndef split_segment_by_ref_buckets(ref_series, s, e, deg_tol):\n    vals = ref_series.to_numpy()[s:e+1]\n    if len(vals)==0 or np.all(np.isnan(vals)): return [(s,e)]\n    buckets = np.floor(vals/float(deg_tol)).astype('float64')\n    mask_good = ~np.isnan(buckets)\n    if mask_good.any():\n        last=None\n        for i in range(len(buckets)):\n            if np.isnan(buckets[i]):\n                buckets[i] = last if last is not None else buckets[mask_good][0]\n            last=buckets[i]\n    segments=[]; start=s; base=buckets[0]\n    for i in range(1,len(buckets)):\n        if buckets[i]!=base:\n            segments.append((start, s+i-1)); start=s+i; base=buckets[i]\n    segments.append((start,e))\n    return segments\n\ndef detect_stability_improved(data, ref_idx, follow_idxs, window=20, std_thr=0.05, diff_thr=0.002, min_len=20, group_by_file=True, split_by_ref_buckets=True, deg_tol=1.0, max_ref_drift=0.3):\n    ref_col=f'T{ref_idx}'; follow_cols=[f'T{i}' for i in follow_idxs]\n    for c in [ref_col]+follow_cols:\n        if c not in data.columns: raise ValueError(f'Missing column: {c}')\n    if group_by_file and 'source_file' in data.columns:\n        first_idx_by_source = (data.index.to_series().groupby(data['source_file']).min().astype(int).to_dict())\n        groups=list(data.groupby('source_file', sort=False))\n    else:\n        first_idx_by_source=None; groups=[('ALL', data)]\n    joint_rows=[]; by_sensor_rows=[]\n    for src, g0 in groups:\n        g=g0.reset_index(drop=True)\n        mask_ref_std = rolling_std_mask(g[ref_col], window, std_thr)\n        mask_ref_slope = rolling_mean_abs_diff_mask(g[ref_col], window, diff_thr)\n        base_ref_mask = (mask_ref_std & mask_ref_slope).to_numpy()\n        masks_others = {c: rolling_std_mask(g[c], window, std_thr).to_numpy() for c in follow_cols}\n        joint_mask = base_ref_mask.copy()\n        for c in follow_cols: joint_mask &= masks_others[c]\n        def _emit_segment(s,e,cols,extra,container):\n            subs=[(s,e)]\n            if split_by_ref_buckets: subs=split_segment_by_ref_buckets(g[ref_col], s, e, deg_tol)\n            for ss,ee in subs:\n                if (ee-ss+1) < int(min_len): continue\n                row={'source_file':src}; row.update(extra); row.update(summarize_interval(g, cols, ss, ee))\n                if src!='ALL' and first_idx_by_source is not None:\n                    first_idx=int(first_idx_by_source.get(src,0)); row['start_idx_abs']=row['start_idx']+first_idx; row['end_idx_abs']=row['end_idx']+first_idx\n                else:\n                    row['start_idx_abs']=row['start_idx']; row['end_idx_abs']=row['end_idx']\n                ref_med=median_level(g[ref_col], ss, ee); row['ref_level']=ref_med; row['drift_ref']=row.get(f'drift_{ref_col}', np.nan)\n                if max_ref_drift is not None and not np.isnan(row['drift_ref']) and row['drift_ref']>max_ref_drift: continue\n                container.append(row)\n        for (s,e) in [(s,e) for (s,e) in segments_from_mask(joint_mask, window) if (e-s+1)>=int(min_len)]:\n            _emit_segment(s,e,[ref_col]+follow_cols, {'ref':ref_col, 'followers':','.join(follow_cols)}, joint_rows)\n        for c in follow_cols:\n            gated = masks_others[c] & base_ref_mask\n            segs=[(s,e) for (s,e) in segments_from_mask(gated, window) if (e-s+1)>=int(min_len)]\n            for (s,e) in segs:\n                _emit_segment(s,e,[c, ref_col], {'sensor':c, 'ref':ref_col}, by_sensor_rows)\n    joint = pd.DataFrame(joint_rows).sort_values(['source_file','start_idx']) if joint_rows else pd.DataFrame()\n    by_sensor = pd.DataFrame(by_sensor_rows).sort_values(['source_file','sensor','start_idx']) if by_sensor_rows else pd.DataFrame()\n    return joint, by_sensor\n\ndef select_longest_per_degree(table, data, ref_idx, mode, deg_tol=1.0, strategy='bucket_centered', centered=True):\n    if table is None or table.empty: return table\n    ref_col=f'T{ref_idx}'; tbl=table.copy()\n    if 'ref_level' not in tbl.columns:\n        ref_levels=[]\n        for _,row in tbl.iterrows():\n            src=row['source_file']; s,e=int(row['start_idx']),int(row['end_idx'])\n            g = data[data['source_file']==src].reset_index(drop=True) if src!='ALL' else data\n            ref_levels.append(median_level(g[ref_col], s, e))\n        tbl['ref_level']=ref_levels\n    group_keys=['source_file','sensor'] if (mode=='by_sensor' and 'sensor' in tbl.columns) else ['source_file']\n    out=[]\n    for _, grp in tbl.sort_values('ref_level').groupby(group_keys, as_index=False):\n        g=grp.sort_values('ref_level').reset_index(drop=True)\n        if strategy in ('bucket','bucket_centered') or centered:\n            bins = np.floor((g['ref_level'] + 0.5*deg_tol)/deg_tol).astype(int) if (strategy=='bucket_centered' or centered) else np.floor(g['ref_level']/deg_tol).astype(int)\n            g=g.assign(_bin=bins)\n            keep = g.sort_values('length', ascending=False).groupby('_bin', as_index=False).head(1)\n            out.append(keep.drop(columns=['_bin']))\n        else:\n            i,n=0,len(g)\n            while i<n:\n                start_val=g.loc[i,'ref_level']; j=i\n                while j+1<n and (g.loc[j+1,'ref_level']-start_val) <= deg_tol: j+=1\n                cluster=g.loc[i:j].copy()\n                keep = cluster.sort_values(['length','end_idx'], ascending=[False,False]).iloc[0:1]\n                out.append(keep); i=j+1\n    return pd.concat(out, ignore_index=True) if out else tbl\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# === 3.1) Виджеты/загрузка — упрощено ===\nprint('Секция виджетов удалена в упрощённом режиме.')\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# === 4) Загрузка через диалог (Colab/Jupyter) ===\n# Делает автодетект среды и НЕ блокирует ядро долгими ожиданиями.\n# Результат: глобалы SELECTED_FILES и IS_BYTES_INPUT.\ntry:\n    if IN_COLAB:\n        # Google Colab: встроенный загрузчик\n        print('Выберите один или несколько .csv/.txt файлов…')\n        uploads = files.upload()\n        SELECTED_FILES = [('uploaded:' + name, content) for name, content in uploads.items()]\n        IS_BYTES_INPUT = True\n        print('Загружено файлов:', len(SELECTED_FILES))\n    else:\n        # Локальный Jupyter: сначала пробуем ipywidgets (не блокирует выполнение)\n        try:\n            import ipywidgets as widgets\n            from IPython.display import display\n            uploader = widgets.FileUpload(accept='.csv,.txt', multiple=True)\n            status = widgets.Output()\n\n            def _extract_files(val):\n                files = []\n                try:\n                    # ipywidgets v7: dict, v8: tuple/list из объектов с атрибутами .name/.content\n                    if isinstance(val, dict):\n                        for _name, _item in val.items():\n                            try:\n                                content = _item.get('content') if isinstance(_item, dict) else getattr(_item, 'content', None)\n                            except Exception:\n                                content = None\n                            if _name is not None and content is not None:\n                                files.append(('uploaded:' + _name, content))\n                    else:\n                        for _it in list(val) if isinstance(val, (list, tuple)) else []:\n                            name2 = getattr(_it, 'name', None) if not isinstance(_it, dict) else _it.get('name')\n                            content2 = getattr(_it, 'content', None) if not isinstance(_it, dict) else _it.get('content')\n                            if name2 is not None and content2 is not None:\n                                files.append(('uploaded:' + name2, content2))\n                except Exception as _e:\n                    with status:\n                        print('Ошибка разборки загруженных файлов:', _e)\n                return files\n\n            def _on_change(change):\n                global SELECTED_FILES, IS_BYTES_INPUT\n                files = _extract_files(uploader.value)\n                if files:\n                    SELECTED_FILES = files\n                    IS_BYTES_INPUT = True\n                    with status:\n                        status.clear_output()\n                        print('Загружено файлов:', len(SELECTED_FILES))\n\n            uploader.observe(_on_change, names='value')\n            display(uploader, status)\n            print('Выберите файлы в виджете выше, затем перейдите к шагу 6.')\n        except Exception as e_wid:\n            # Фолбэк: системный диалог (может не работать в headless)\n            try:\n                import tkinter as tk\n                from tkinter import filedialog\n                root = tk.Tk(); root.withdraw()\n                print('Откроется системное окно выбора файлов…')\n                paths = filedialog.askopenfilenames(title='Выберите .csv/.txt', filetypes=[('CSV/TXT','*.csv *.txt'), ('All','*.*')])\n                paths = list(paths)\n                if paths:\n                    SELECTED_FILES = paths\n                    IS_BYTES_INPUT = False\n                    print('Выбрано файлов:', len(SELECTED_FILES))\n                else:\n                    print('Диалог закрыт без выбора. Повторите или используйте шаг 5.')\n            except Exception as e_tk:\n                print('Не удалось открыть диалог выбора файлов. Сообщение:', e_tk)\n                print('В качестве альтернативы можно указать пути вручную и сразу перейти к шагу 6.')\nexcept Exception as e:\n    print('Ошибка при загрузке через диалог:', e)\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# === 5) Google Drive — упрощено ===\nprint('Секция Google Drive пропущена в упрощённом режиме.')\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# === 6) Загрузка данных ===\nframes, errors = [], []\nimport os, io\nif os.path.exists('combined_temperatures.csv'):\n    data = pd.read_csv('combined_temperatures.csv')\n    DATA = data.copy()\n    print('Загружен combined_temperatures.csv:', data.shape)\nelse:\n    assert 'SELECTED_FILES' in globals(), 'Выберите файлы (секция 4).'\n    for item in SELECTED_FILES:\n        try:\n            if isinstance(item, tuple) and len(item)==2:\n                name, content = item\n                bio = io.BytesIO(content)\n                df = read_one_table(name, bio, date_format=DATE_FORMAT or None)\n            else:\n                path = item\n                with open(path, 'rb') as f:\n                    df = read_one_table(os.path.basename(path), io.BytesIO(f.read()), date_format=DATE_FORMAT or None)\n            frames.append(df)\n        except Exception as e:\n            errors.append((str(item), str(e)))\n    if not frames:\n        raise RuntimeError('Не удалось прочитать ни один файл.')\n    data = pd.concat(frames, ignore_index=True).sort_values('date').reset_index(drop=True)\n    follow_idxs = [REF_IDX + i for i in range(1, N_FOLLOW + 1) if REF_IDX + i <= 15]\n    cols_keep = ['date', f'T{REF_IDX}'] + [f'T{i}' for i in follow_idxs] + ['source_file']\n    data = data[[c for c in cols_keep if c in data.columns]]\n    DATA = data\n    print('Собранный датафрейм:', data.shape)\ndisplay(DATA.head(10))\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# --- Guard: ensure stability functions exist ---\nif 'detect_stability_improved' not in globals() or 'select_longest_per_degree' not in globals():\n    import numpy as np\n    import pandas as pd\n    from typing import List, Tuple, Sequence, Dict, Optional\n\n    def rolling_std_mask(series, window, threshold):\n        rs = series.rolling(window=window, min_periods=window).std()\n        return (rs <= threshold)\n\n    def rolling_mean_abs_diff_mask(series, window, diff_threshold):\n        d = series.diff().abs()\n        m = d.rolling(window=window, min_periods=window).mean()\n        return (m <= diff_threshold)\n\n    def segments_from_mask(mask, window):\n        import numpy as _np\n        arr = mask.to_numpy() if hasattr(mask, 'to_numpy') else _np.asarray(mask)\n        segs = []; cur=None\n        for i, ok in enumerate(arr):\n            if ok:\n                s = max(0, i - window + 1); e = i\n                if cur is None: cur=(s,e)\n                else:\n                    cs, ce = cur\n                    if s <= ce + 1: cur=(cs, max(ce, e))\n                    else: segs.append(cur); cur=(s,e)\n            else:\n                if cur is not None: segs.append(cur); cur=None\n        if cur is not None: segs.append(cur)\n        return segs\n\n    def summarize_interval(df, cols, s, e):\n        row={'start_idx':int(s),'end_idx':int(e),'length':int(e-s+1)}\n        row['start_date']=pd.to_datetime(df.loc[s,'date']) if s < len(df) else pd.NaT\n        row['end_date']=pd.to_datetime(df.loc[e,'date']) if e < len(df) else pd.NaT\n        for c in cols:\n            vals = df[c].to_numpy()[s:e+1]\n            good = ~np.isnan(vals)\n            row[f'mean_{c}'] = float(np.nanmean(vals)) if good.any() else np.nan\n            row[f'std_{c}'] = float(np.nanstd(vals, ddof=1)) if good.sum()>1 else np.nan\n            row[f'min_{c}'] = float(np.nanmin(vals)) if good.any() else np.nan\n            row[f'max_{c}'] = float(np.nanmax(vals)) if good.any() else np.nan\n            row[f'drift_{c}'] = row[f'max_{c}'] - row[f'min_{c}'] if good.any() else np.nan\n        return row\n\n    def median_level(series, s, e):\n        vals = series.to_numpy()[int(s):int(e)+1]\n        return float(np.nanmedian(vals))\n\n    def split_segment_by_ref_buckets(ref_series, s, e, deg_tol):\n        vals = ref_series.to_numpy()[s:e+1]\n        if len(vals)==0 or np.all(np.isnan(vals)): return [(s,e)]\n        buckets = np.floor(vals/float(deg_tol)).astype('float64')\n        mask_good = ~np.isnan(buckets)\n        if mask_good.any():\n            last=None\n            for i in range(len(buckets)):\n                if np.isnan(buckets[i]):\n                    buckets[i] = last if last is not None else buckets[mask_good][0]\n                last=buckets[i]\n        segments=[]; start=s; base=buckets[0]\n        for i in range(1,len(buckets)):\n            if buckets[i]!=base:\n                segments.append((start, s+i-1)); start=s+i; base=buckets[i]\n        segments.append((start,e))\n        return segments\n\n    def detect_stability_improved(data, ref_idx, follow_idxs, window=20, std_thr=0.05, diff_thr=0.002, min_len=20, group_by_file=True, split_by_ref_buckets=True, deg_tol=1.0, max_ref_drift=0.3):\n        ref_col=f'T{ref_idx}'; follow_cols=[f'T{i}' for i in follow_idxs]\n        for c in [ref_col]+follow_cols:\n            if c not in data.columns: raise ValueError(f'Missing column: {c}')\n        if group_by_file and 'source_file' in data.columns:\n            first_idx_by_source = (data.index.to_series().groupby(data['source_file']).min().astype(int).to_dict())\n            groups=list(data.groupby('source_file', sort=False))\n        else:\n            first_idx_by_source=None; groups=[('ALL', data)]\n        joint_rows=[]; by_sensor_rows=[]\n        for src, g0 in groups:\n            g=g0.reset_index(drop=True)\n            mask_ref_std = rolling_std_mask(g[ref_col], window, std_thr)\n            mask_ref_slope = rolling_mean_abs_diff_mask(g[ref_col], window, diff_thr)\n            base_ref_mask = (mask_ref_std & mask_ref_slope).to_numpy()\n            masks_others = {c: rolling_std_mask(g[c], window, std_thr).to_numpy() for c in follow_cols}\n            joint_mask = base_ref_mask.copy()\n            for c in follow_cols: joint_mask &= masks_others[c]\n            def _emit_segment(s,e,cols,extra,container):\n                subs=[(s,e)]\n                if split_by_ref_buckets: subs=split_segment_by_ref_buckets(g[ref_col], s, e, deg_tol)\n                for ss,ee in subs:\n                    if (ee-ss+1) < int(min_len): continue\n                    row={'source_file':src}; row.update(extra); row.update(summarize_interval(g, cols, ss, ee))\n                    if src!='ALL' and first_idx_by_source is not None:\n                        first_idx=int(first_idx_by_source.get(src,0)); row['start_idx_abs']=row['start_idx']+first_idx; row['end_idx_abs']=row['end_idx']+first_idx\n                    else:\n                        row['start_idx_abs']=row['start_idx']; row['end_idx_abs']=row['end_idx']\n                    ref_med=median_level(g[ref_col], ss, ee); row['ref_level']=ref_med; row['drift_ref']=row.get(f'drift_{ref_col}', np.nan)\n                    if max_ref_drift is not None and not np.isnan(row['drift_ref']) and row['drift_ref']>max_ref_drift: continue\n                    container.append(row)\n            for (s,e) in [(s,e) for (s,e) in segments_from_mask(joint_mask, window) if (e-s+1)>=int(min_len)]:\n                _emit_segment(s,e,[ref_col]+follow_cols, {'ref':ref_col, 'followers':','.join(follow_cols)}, joint_rows)\n            for c in follow_cols:\n                gated = masks_others[c] & base_ref_mask\n                segs=[(s,e) for (s,e) in segments_from_mask(gated, window) if (e-s+1)>=int(min_len)]\n                for (s,e) in segs:\n                    _emit_segment(s,e,[c, ref_col], {'sensor':c, 'ref':ref_col}, by_sensor_rows)\n        joint = pd.DataFrame(joint_rows).sort_values(['source_file','start_idx']) if joint_rows else pd.DataFrame()\n        by_sensor = pd.DataFrame(by_sensor_rows).sort_values(['source_file','sensor','start_idx']) if by_sensor_rows else pd.DataFrame()\n        return joint, by_sensor\n\n    def select_longest_per_degree(table, data, ref_idx, mode, deg_tol=1.0, strategy='bucket_centered', centered=True):\n        if table is None or table.empty: return table\n        ref_col=f'T{ref_idx}'; tbl=table.copy()\n        if 'ref_level' not in tbl.columns:\n            ref_levels=[]\n            for _,row in tbl.iterrows():\n                src=row['source_file']; s,e=int(row['start_idx']),int(row['end_idx'])\n                g = data[data['source_file']==src].reset_index(drop=True) if src!='ALL' else data\n                ref_levels.append(median_level(g[ref_col], s, e))\n            tbl['ref_level']=ref_levels\n        group_keys=['source_file','sensor'] if (mode=='by_sensor' and 'sensor' in tbl.columns) else ['source_file']\n        out=[]\n        for _, grp in tbl.sort_values('ref_level').groupby(group_keys, as_index=False):\n            g=grp.sort_values('ref_level').reset_index(drop=True)\n            if strategy in ('bucket','bucket_centered') or centered:\n                bins = np.floor((g['ref_level'] + 0.5*deg_tol)/deg_tol).astype(int) if (strategy=='bucket_centered' or centered) else np.floor(g['ref_level']/deg_tol).astype(int)\n                g=g.assign(_bin=bins)\n                keep = g.sort_values('length', ascending=False).groupby('_bin', as_index=False).head(1)\n                out.append(keep.drop(columns=['_bin']))\n            else:\n                i,n=0,len(g)\n                while i<n:\n                    start_val=g.loc[i,'ref_level']; j=i\n                    while j+1<n and (g.loc[j+1,'ref_level']-start_val) <= deg_tol: j+=1\n                    cluster=g.loc[i:j].copy()\n                    keep = cluster.sort_values(['length','end_idx'], ascending=[False,False]).iloc[0:1]\n                    out.append(keep); i=j+1\n        return pd.concat(out, ignore_index=True) if out else tbl\n\n# === 7) Поиск стабильных интервалов (улучшенный) ===\nassert DATA is not None, 'Нет DATA.'\nfollow_idxs = [REF_IDX + i for i in range(1, N_FOLLOW + 1) if REF_IDX + i <= 15]\nSTABLE_JOINT, STABLE_BY_SENSOR = detect_stability_improved(\n    DATA, ref_idx=REF_IDX, follow_idxs=follow_idxs,\n    window=WINDOW_N, std_thr=STD_THR, diff_thr=DIFF_THR,\n    min_len=MIN_LEN, group_by_file=GROUP_BY_FILE,\n    split_by_ref_buckets=True, deg_tol=DEG_TOL, max_ref_drift=MAX_REF_DRIFT\n)\nprint('Обнаружено интервалов: совместных =', 0 if STABLE_JOINT is None else len(STABLE_JOINT),\n      '; по датчику =', 0 if STABLE_BY_SENSOR is None else len(STABLE_BY_SENSOR))\nif STABLE_BY_SENSOR is not None and not STABLE_BY_SENSOR.empty:\n    display(STABLE_BY_SENSOR.head(5))\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## 7.1 Отбор плато: самое длинное в пределах ±DEG_TOL °C по эталону\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# --- Guard: ensure stability functions exist ---\nif 'detect_stability_improved' not in globals() or 'select_longest_per_degree' not in globals():\n    import numpy as np\n    import pandas as pd\n    from typing import List, Tuple, Sequence, Dict, Optional\n\n    def rolling_std_mask(series, window, threshold):\n        rs = series.rolling(window=window, min_periods=window).std()\n        return (rs <= threshold)\n\n    def rolling_mean_abs_diff_mask(series, window, diff_threshold):\n        d = series.diff().abs()\n        m = d.rolling(window=window, min_periods=window).mean()\n        return (m <= diff_threshold)\n\n    def segments_from_mask(mask, window):\n        import numpy as _np\n        arr = mask.to_numpy() if hasattr(mask, 'to_numpy') else _np.asarray(mask)\n        segs = []; cur=None\n        for i, ok in enumerate(arr):\n            if ok:\n                s = max(0, i - window + 1); e = i\n                if cur is None: cur=(s,e)\n                else:\n                    cs, ce = cur\n                    if s <= ce + 1: cur=(cs, max(ce, e))\n                    else: segs.append(cur); cur=(s,e)\n            else:\n                if cur is not None: segs.append(cur); cur=None\n        if cur is not None: segs.append(cur)\n        return segs\n\n    def summarize_interval(df, cols, s, e):\n        row={'start_idx':int(s),'end_idx':int(e),'length':int(e-s+1)}\n        row['start_date']=pd.to_datetime(df.loc[s,'date']) if s < len(df) else pd.NaT\n        row['end_date']=pd.to_datetime(df.loc[e,'date']) if e < len(df) else pd.NaT\n        for c in cols:\n            vals = df[c].to_numpy()[s:e+1]\n            good = ~np.isnan(vals)\n            row[f'mean_{c}'] = float(np.nanmean(vals)) if good.any() else np.nan\n            row[f'std_{c}'] = float(np.nanstd(vals, ddof=1)) if good.sum()>1 else np.nan\n            row[f'min_{c}'] = float(np.nanmin(vals)) if good.any() else np.nan\n            row[f'max_{c}'] = float(np.nanmax(vals)) if good.any() else np.nan\n            row[f'drift_{c}'] = row[f'max_{c}'] - row[f'min_{c}'] if good.any() else np.nan\n        return row\n\n    def median_level(series, s, e):\n        vals = series.to_numpy()[int(s):int(e)+1]\n        return float(np.nanmedian(vals))\n\n    def split_segment_by_ref_buckets(ref_series, s, e, deg_tol):\n        vals = ref_series.to_numpy()[s:e+1]\n        if len(vals)==0 or np.all(np.isnan(vals)): return [(s,e)]\n        buckets = np.floor(vals/float(deg_tol)).astype('float64')\n        mask_good = ~np.isnan(buckets)\n        if mask_good.any():\n            last=None\n            for i in range(len(buckets)):\n                if np.isnan(buckets[i]):\n                    buckets[i] = last if last is not None else buckets[mask_good][0]\n                last=buckets[i]\n        segments=[]; start=s; base=buckets[0]\n        for i in range(1,len(buckets)):\n            if buckets[i]!=base:\n                segments.append((start, s+i-1)); start=s+i; base=buckets[i]\n        segments.append((start,e))\n        return segments\n\n    def detect_stability_improved(data, ref_idx, follow_idxs, window=20, std_thr=0.05, diff_thr=0.002, min_len=20, group_by_file=True, split_by_ref_buckets=True, deg_tol=1.0, max_ref_drift=0.3):\n        ref_col=f'T{ref_idx}'; follow_cols=[f'T{i}' for i in follow_idxs]\n        for c in [ref_col]+follow_cols:\n            if c not in data.columns: raise ValueError(f'Missing column: {c}')\n        if group_by_file and 'source_file' in data.columns:\n            first_idx_by_source = (data.index.to_series().groupby(data['source_file']).min().astype(int).to_dict())\n            groups=list(data.groupby('source_file', sort=False))\n        else:\n            first_idx_by_source=None; groups=[('ALL', data)]\n        joint_rows=[]; by_sensor_rows=[]\n        for src, g0 in groups:\n            g=g0.reset_index(drop=True)\n            mask_ref_std = rolling_std_mask(g[ref_col], window, std_thr)\n            mask_ref_slope = rolling_mean_abs_diff_mask(g[ref_col], window, diff_thr)\n            base_ref_mask = (mask_ref_std & mask_ref_slope).to_numpy()\n            masks_others = {c: rolling_std_mask(g[c], window, std_thr).to_numpy() for c in follow_cols}\n            joint_mask = base_ref_mask.copy()\n            for c in follow_cols: joint_mask &= masks_others[c]\n            def _emit_segment(s,e,cols,extra,container):\n                subs=[(s,e)]\n                if split_by_ref_buckets: subs=split_segment_by_ref_buckets(g[ref_col], s, e, deg_tol)\n                for ss,ee in subs:\n                    if (ee-ss+1) < int(min_len): continue\n                    row={'source_file':src}; row.update(extra); row.update(summarize_interval(g, cols, ss, ee))\n                    if src!='ALL' and first_idx_by_source is not None:\n                        first_idx=int(first_idx_by_source.get(src,0)); row['start_idx_abs']=row['start_idx']+first_idx; row['end_idx_abs']=row['end_idx']+first_idx\n                    else:\n                        row['start_idx_abs']=row['start_idx']; row['end_idx_abs']=row['end_idx']\n                    ref_med=median_level(g[ref_col], ss, ee); row['ref_level']=ref_med; row['drift_ref']=row.get(f'drift_{ref_col}', np.nan)\n                    if max_ref_drift is not None and not np.isnan(row['drift_ref']) and row['drift_ref']>max_ref_drift: continue\n                    container.append(row)\n            for (s,e) in [(s,e) for (s,e) in segments_from_mask(joint_mask, window) if (e-s+1)>=int(min_len)]:\n                _emit_segment(s,e,[ref_col]+follow_cols, {'ref':ref_col, 'followers':','.join(follow_cols)}, joint_rows)\n            for c in follow_cols:\n                gated = masks_others[c] & base_ref_mask\n                segs=[(s,e) for (s,e) in segments_from_mask(gated, window) if (e-s+1)>=int(min_len)]\n                for (s,e) in segs:\n                    _emit_segment(s,e,[c, ref_col], {'sensor':c, 'ref':ref_col}, by_sensor_rows)\n        joint = pd.DataFrame(joint_rows).sort_values(['source_file','start_idx']) if joint_rows else pd.DataFrame()\n        by_sensor = pd.DataFrame(by_sensor_rows).sort_values(['source_file','sensor','start_idx']) if by_sensor_rows else pd.DataFrame()\n        return joint, by_sensor\n\n    def select_longest_per_degree(table, data, ref_idx, mode, deg_tol=1.0, strategy='bucket_centered', centered=True):\n        if table is None or table.empty: return table\n        ref_col=f'T{ref_idx}'; tbl=table.copy()\n        if 'ref_level' not in tbl.columns:\n            ref_levels=[]\n            for _,row in tbl.iterrows():\n                src=row['source_file']; s,e=int(row['start_idx']),int(row['end_idx'])\n                g = data[data['source_file']==src].reset_index(drop=True) if src!='ALL' else data\n                ref_levels.append(median_level(g[ref_col], s, e))\n            tbl['ref_level']=ref_levels\n        group_keys=['source_file','sensor'] if (mode=='by_sensor' and 'sensor' in tbl.columns) else ['source_file']\n        out=[]\n        for _, grp in tbl.sort_values('ref_level').groupby(group_keys, as_index=False):\n            g=grp.sort_values('ref_level').reset_index(drop=True)\n            if strategy in ('bucket','bucket_centered') or centered:\n                bins = np.floor((g['ref_level'] + 0.5*deg_tol)/deg_tol).astype(int) if (strategy=='bucket_centered' or centered) else np.floor(g['ref_level']/deg_tol).astype(int)\n                g=g.assign(_bin=bins)\n                keep = g.sort_values('length', ascending=False).groupby('_bin', as_index=False).head(1)\n                out.append(keep.drop(columns=['_bin']))\n            else:\n                i,n=0,len(g)\n                while i<n:\n                    start_val=g.loc[i,'ref_level']; j=i\n                    while j+1<n and (g.loc[j+1,'ref_level']-start_val) <= deg_tol: j+=1\n                    cluster=g.loc[i:j].copy()\n                    keep = cluster.sort_values(['length','end_idx'], ascending=[False,False]).iloc[0:1]\n                    out.append(keep); i=j+1\n        return pd.concat(out, ignore_index=True) if out else tbl\n\n# === 8) Отбор по полочкам: один самый длинный интервал на корзину ===\nassert DATA is not None, 'Нет DATA.'\nif STABLE_BY_SENSOR is not None and not STABLE_BY_SENSOR.empty:\n    STABLE_BY_SENSOR = select_longest_per_degree(STABLE_BY_SENSOR, DATA, REF_IDX, mode='by_sensor', deg_tol=DEG_TOL, strategy='bucket_centered', centered=True)\n    print('STABLE_BY_SENSOR после отбора:', len(STABLE_BY_SENSOR))\n    display(STABLE_BY_SENSOR.head(10))\nif STABLE_JOINT is not None and not STABLE_JOINT.empty:\n    STABLE_JOINT = select_longest_per_degree(STABLE_JOINT, DATA, REF_IDX, mode='joint', deg_tol=DEG_TOL, strategy='bucket_centered', centered=True)\n    print('STABLE_JOINT после отбора:', len(STABLE_JOINT))\n    display(STABLE_JOINT.head(10))\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# === 9) Калибровочная таблица (X–Y и ошибки) ===\nassert DATA is not None, 'Нет DATA.'\nref_col = f'T{REF_IDX}'\nrows=[]\nif STABLE_BY_SENSOR is not None and not STABLE_BY_SENSOR.empty:\n    for _, r in STABLE_BY_SENSOR.iterrows():\n        sensor = r['sensor']\n        x_mean = r.get(f'mean_{sensor}', None)\n        x_std  = r.get(f'std_{sensor}', None)\n        y_mean = r.get(f'mean_{ref_col}', None)\n        y_std  = r.get(f'std_{ref_col}', None)\n        ref_level = r.get('ref_level', None)\n        length = int(r.get('length', 0))\n        start_date = r.get('start_date', None)\n        end_date   = r.get('end_date', None)\n        src = r.get('source_file', None)\n        if ref_level is not None and not pd.isna(ref_level):\n            bin_center = float(np.round(ref_level / DEG_TOL) * DEG_TOL)\n            bin_low = bin_center - 0.5*DEG_TOL\n            bin_high = bin_center + 0.5*DEG_TOL\n        else:\n            bin_center = np.nan; bin_low=np.nan; bin_high=np.nan\n        rows.append({\n            'source_file': src, 'sensor': sensor, 'ref': ref_col,\n            'bin_center': bin_center, 'bin_low': bin_low, 'bin_high': bin_high,\n            'x_mean': x_mean, 'x_std': x_std, 'y_mean': y_mean, 'y_std': y_std,\n            'n_samples': length, 'start_date': start_date, 'end_date': end_date\n        })\ncalibration_points_by_sensor = pd.DataFrame(rows).dropna(subset=['x_mean','y_mean']).reset_index(drop=True)\nprint('Калибровочных точек:', len(calibration_points_by_sensor))\ndisplay(calibration_points_by_sensor.head(10))\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# === 9.1) Сохранение калибровочной таблицы ===\nout_dir = os.getcwd()\ncalibration_points_by_sensor.to_csv(os.path.join(out_dir, 'calibration_points_by_sensor.csv'), index=False)\nprint('Сохранено:', os.path.join(out_dir, 'calibration_points_by_sensor.csv'))\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# === 10) Быстрый график калибровочных точек с ошибками ===\nimport matplotlib.pyplot as plt\nimport numpy as np\nassert 'calibration_points_by_sensor' in globals() and not calibration_points_by_sensor.empty, 'Нет калибровочной таблицы.'\nfor sensor, g in calibration_points_by_sensor.groupby('sensor'):\n    g = g.sort_values('bin_center')\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(7,6), gridspec_kw={'height_ratios':[3,2]})\n    ax1.errorbar(g['x_mean'], g['y_mean'], xerr=g['x_std'], yerr=g['y_std'], fmt='o', capsize=3)\n    ax1.set_xlabel(f'{sensor} (X)')\n    ax1.set_ylabel(f'T{REF_IDX} (Y)')\n    ax1.set_title(f'Калибровочные точки: {sensor}')\n    ax1.grid(True, alpha=0.3)\n    width=0.35; idx=np.arange(len(g))\n    ax2.bar(idx - width/2, g['x_std'], width, label='std X')\n    ax2.bar(idx + width/2, g['y_std'], width, label='std Y')\n    ax2.set_xticks(idx)\n    ax2.set_xticklabels([f'{bc:.1f}' for bc in g['bin_center']])\n    ax2.set_xlabel('Центр полочки (°C)')\n    ax2.set_ylabel('Std (°C)')\n    ax2.legend(); ax2.grid(True, axis='y', alpha=0.3)\n    plt.tight_layout(); plt.show()\n"}, {"cell_type": "code", "metadata": {}, "source": "# === 11) Калибровочные кривые (полиномы) ===\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nassert 'calibration_points_by_sensor' in globals() and not calibration_points_by_sensor.empty, 'Нет калибровочной таблицы.'\n\ndef _poly_formula(coeffs):\n    # coeffs in descending order: c0*x^n + c1*x^(n-1) + ... + cN\n    deg = len(coeffs) - 1\n    terms = []\n    for i, c in enumerate(coeffs):\n        p = deg - i\n        if abs(c) < 1e-15:\n            continue\n        coef = f\"{c:.8g}\"\n        if p == 0:\n            term = f\"{coef}\"\n        elif p == 1:\n            term = f\"{coef}·X\"\n        else:\n            term = f\"{coef}·X^{p}\"\n        terms.append(term)\n    if not terms:\n        return 'Y = 0'\n    s = ' + '.join(terms)\n    s = s.replace('+ -','- ')\n    return 'Y = ' + s\n\nfor sensor, g in calibration_points_by_sensor.groupby('sensor'):\n    g = g.sort_values('x_mean').reset_index(drop=True)\n    x = g['x_mean'].to_numpy(dtype=float)\n    y = g['y_mean'].to_numpy(dtype=float)\n    y_std = g['y_std'].to_numpy(dtype=float) if 'y_std' in g else np.full_like(y, np.nan)\n    w = 1.0 / np.clip(y_std, 1e-9, np.inf)\n    n = len(x)\n    deg_max = max(1, n - 2)\n\n    fits = []\n    for d in range(1, deg_max + 1):\n        try:\n            coefs = np.polyfit(x, y, deg=d, w=w)\n            p = np.poly1d(coefs)\n            yhat = p(x)\n            resid = y - yhat\n            rmse = float(np.sqrt(np.mean(resid**2)))\n            mae = float(np.mean(np.abs(resid)))\n            mx = float(np.max(np.abs(resid)))\n            fits.append({'deg': d, 'coefs': coefs, 'poly': p, 'rmse': rmse, 'mae': mae, 'maxerr': mx, 'resid': resid})\n        except Exception as e:\n            print(f\"[warn] {sensor}: не удалось аппроксимировать степенью {d}: {e}\")\n\n    print(f\"\nСенсор {sensor}: {n} точек; степени 1..{deg_max}\")\n    for f in fits:\n        cdesc = ', '.join([f\"{c:.8g}\" for c in f['coefs']])\n        print(f\"deg={f['deg']}: {_poly_formula(f['coefs'])}\")\n        print(f\"    coeffs (старшая→младшая): [{cdesc}] | RMSE={f['rmse']:.6g}, MAE={f['mae']:.6g}, Max|err|={f['maxerr']:.6g}\")\n\n    # Визуализация: X–Y с кривыми и остатки по степеням\n    xs = np.linspace(float(x.min()), float(x.max()), 200)\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6), gridspec_kw={'height_ratios': [3, 2]})\n\n    # Верх: точки с ошибками и две кривые (deg=1 и deg_max)\n    ax1.errorbar(x, y, xerr=g.get('x_std', pd.Series(index=g.index, dtype=float)), yerr=g.get('y_std', pd.Series(index=g.index, dtype=float)), fmt='o', capsize=3, label='точки')\n    p1 = next((f['poly'] for f in fits if f['deg'] == 1), None)\n    pN = max(fits, key=lambda f: f['deg'])['poly'] if fits else None\n    if p1 is not None:\n        ax1.plot(xs, p1(xs), label='deg=1')\n    if pN is not None and (p1 is None or pN.order != 1):\n        ax1.plot(xs, pN(xs), label=f\"deg={pN.order}\")\n    ax1.set_xlabel(f\"{sensor} (X)\")\n    ax1.set_ylabel(f\"T{REF_IDX} (Y)\")\n    ax1.set_title(f\"Калибровка: {sensor}\")\n    ax1.grid(True, alpha=0.3)\n    ax1.legend()\n\n    # Низ: остатки Y - Y_hat для каждой степени (кластерные столбцы)\n    idx = np.arange(n)\n    width = 0.8 / max(len(fits), 1)\n    for k, f in enumerate(fits):\n        ax2.bar(idx + (k - (len(fits)-1)/2)*width, f['resid'], width, label=f\"deg={f['deg']}\")\n    ax2.axhline(0, color='k', lw=0.8)\n    if 'bin_center' in g.columns:\n        labels = [f\"{bc:.1f}\" for bc in g['bin_center']]\n    else:\n        labels = [f\"{xi:.2f}\" for xi in x]\n    ax2.set_xticks(idx)\n    ax2.set_xticklabels(labels)\n    ax2.set_xlabel('Полочка (центр)')\n    ax2.set_ylabel('Y - Y_hat (°C)')\n    ax2.legend(ncol=min(4, len(fits)))\n    ax2.grid(True, axis='y', alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n", "outputs": [], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": "# === 12) Калибровочные модели (L2 и L_inf) ===\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nassert 'calibration_points_by_sensor' in globals() and not calibration_points_by_sensor.empty, 'Нет калибровочной таблицы.'\n\ntry:\n    from scipy.optimize import linprog  # optional, для точной L_inf\n    _HAS_SCIPY = True\nexcept Exception:\n    _HAS_SCIPY = False\n\ndef _build_vandermonde(x, deg):\n    # Descending powers\n    return np.vander(x, N=deg+1, increasing=False)\n\ndef _poly_formula(coeffs):\n    deg = len(coeffs) - 1\n    parts = []\n    for i, c in enumerate(coeffs):\n        p = deg - i\n        if abs(c) < 1e-15:\n            continue\n        coef = f\"{c:.8g}\"\n        if p == 0:\n            parts.append(f\"{coef}\")\n        elif p == 1:\n            parts.append(f\"{coef}·X\")\n        else:\n            parts.append(f\"{coef}·X^{p}\")\n    if not parts:\n        return 'Y = 0'\n    expr = ' + '.join(parts).replace('+ -', '- ')\n    return 'Y = ' + expr\n\ndef fit_L2(x, y, deg, y_std=None):\n    if y_std is not None:\n        w = 1.0 / np.clip(y_std, 1e-12, np.inf)\n        coefs = np.polyfit(x, y, deg=deg, w=w)\n    else:\n        coefs = np.polyfit(x, y, deg=deg)\n    p = np.poly1d(coefs)\n    yhat = p(x)\n    resid = y - yhat\n    return coefs, p, resid\n\ndef fit_Linf(x, y, deg, y_std=None, max_iter_irls=25):\n    # Минимакс через LP, если доступен SciPy. Иначе — IRLS-приближение к L_inf.\n    if _HAS_SCIPY:\n        X = _build_vandermonde(x, deg)\n        if y_std is not None:\n            w = 1.0 / np.clip(y_std, 1e-12, np.inf)\n            Xw = X * w[:, None]\n            yw = y * w\n        else:\n            Xw = X\n            yw = y\n        n, m = Xw.shape\n        # Variables: c (m coeffs) and t (scalar)\n        # Minimize t subject to:  X c - t <= y;  -X c - t <= -y\n        # Build A_ub, b_ub for z=[c;t]\n        A1 = np.hstack([Xw, -np.ones((n,1))])\n        b1 = yw\n        A2 = np.hstack([-Xw, -np.ones((n,1))])\n        b2 = -yw\n        A_ub = np.vstack([A1, A2])\n        b_ub = np.concatenate([b1, b2])\n        c_vec = np.zeros(m+1); c_vec[-1] = 1.0\n        bounds = [(None, None)]*m + [(0, None)]\n        res = linprog(c_vec, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n        if not res.success:\n            raise RuntimeError('linprog failed: ' + str(res.message))\n        coefs = res.x[:-1]\n        p = np.poly1d(coefs)\n        yhat = p(x)\n        resid = y - yhat\n        return coefs, p, resid\n    # Fallback IRLS to approximate L_inf: повышаем веса у больших остатков\n    # Старт — L2\n    coefs = np.polyfit(x, y, deg=deg)\n    for _ in range(max_iter_irls):\n        p = np.poly1d(coefs)\n        resid = y - p(x)\n        s = np.clip(np.abs(resid), 1e-9, None)\n        # scale by std if provided\n        if y_std is not None:\n            s = s / np.clip(y_std, 1e-12, np.inf)\n        w = 1.0 / s\n        coefs = np.polyfit(x, y, deg=deg, w=w)\n    p = np.poly1d(coefs)\n    resid = y - p(x)\n    return coefs, p, resid\n\nrows = []\n\nfor sensor, g in calibration_points_by_sensor.groupby('sensor'):\n    g = g.sort_values('x_mean').reset_index(drop=True)\n    x = g['x_mean'].to_numpy(float)\n    y = g['y_mean'].to_numpy(float)\n    y_std = g['y_std'].to_numpy(float) if 'y_std' in g else None\n    n = len(x)\n    deg_max = max(1, n - 2)\n\n    model_bank = []\n    # L2 and L_inf for each degree\n    for d in range(1, deg_max+1):\n        # L2\n        c2, p2, r2 = fit_L2(x, y, d, y_std)\n        rmse2 = float(np.sqrt(np.mean(r2**2)))\n        mae2 = float(np.mean(np.abs(r2)))\n        mx2 = float(np.max(np.abs(r2)))\n        model_bank.append({'sensor':sensor,'method':'L2','deg':d,'coefs':c2,'poly':p2,'resid':r2,'rmse':rmse2,'mae':mae2,'maxerr':mx2})\n        # L_inf\n        cI, pI, rI = fit_Linf(x, y, d, y_std)\n        rmseI = float(np.sqrt(np.mean(rI**2)))\n        maeI = float(np.mean(np.abs(rI)))\n        mxI = float(np.max(np.abs(rI)))\n        model_bank.append({'sensor':sensor,'method':'L_inf','deg':d,'coefs':cI,'poly':pI,'resid':rI,'rmse':rmseI,'mae':maeI,'maxerr':mxI})\n\n    # Печать формул и метрик\n    print(f\"\nСенсор {sensor}: {n} точек; степени 1..{deg_max}\")\n    for f in model_bank:\n        cdesc = ', '.join([f\"{c:.8g}\" for c in f['coefs']])\n        print(f\"{f['method']} deg={f['deg']}: {_poly_formula(f['coefs'])}\")\n        print(f\"    coeffs (старшая→младшая): [{cdesc}] | RMSE={f['rmse']:.6g}, MAE={f['mae']:.6g}, Max|err|={f['maxerr']:.6g}\")\n\n    # Таблица коэффициентов\n    for f in model_bank:\n        row = {\n            'sensor': f['sensor'],\n            'method': f['method'],\n            'degree': f['deg'],\n            'n_points': n,\n            'rmse': f['rmse'],\n            'mae': f['mae'],\n            'maxerr': f['maxerr'],\n            'formula': _poly_formula(f['coefs'])\n        }\n        # Разложим по степеням: c_p{p}\n        d = f['deg']\n        for i, c in enumerate(f['coefs']):\n            pwr = d - i\n            row[f'c_p{pwr}'] = float(c)\n        rows.append(row)\n\n    # Графики: отдельно для L2 и L_inf (deg=1 и deg=deg_max)\n    xs = np.linspace(float(x.min()), float(x.max()), 200)\n    fig, axes = plt.subplots(2, 2, figsize=(11, 7), gridspec_kw={'height_ratios':[3,2]})\n    (ax1, ax2), (ax3, ax4) = axes\n\n    # L2 — точки и кривые\n    ax1.errorbar(x, y,\n                 xerr=g.get('x_std', pd.Series(index=g.index, dtype=float)),\n                 yerr=g.get('y_std', pd.Series(index=g.index, dtype=float)),\n                 fmt='o', capsize=3, label='точки')\n    p2_1 = next((f['poly'] for f in model_bank if f['method']=='L2' and f['deg']==1), None)\n    p2_N = next((f['poly'] for f in model_bank if f['method']=='L2' and f['deg']==deg_max), None)\n    if p2_1 is not None: ax1.plot(xs, p2_1(xs), label='L2 deg=1')\n    if p2_N is not None and (p2_1 is None or p2_N.order != 1): ax1.plot(xs, p2_N(xs), label=f'L2 deg={p2_N.order}')\n    ax1.set_title(f'{sensor}: аппроксимация L2')\n    ax1.set_xlabel(f'{sensor} (X)'); ax1.set_ylabel(f'T{REF_IDX} (Y)'); ax1.grid(True, alpha=0.3); ax1.legend()\n\n    # L2 — остатки\n    idx = np.arange(n); width = 0.8 / max(deg_max,1)\n    for d in range(1, deg_max+1):\n        r = next(f['resid'] for f in model_bank if f['method']=='L2' and f['deg']==d)\n        ax2.bar(idx + (d-1 - (deg_max-1)/2)*width, r, width, label=f'deg={d}')\n    ax2.axhline(0, color='k', lw=0.8)\n    labels = [f\"{bc:.1f}\" for bc in g['bin_center']] if 'bin_center' in g else [f\"{xi:.2f}\" for xi in x]\n    ax2.set_xticks(idx); ax2.set_xticklabels(labels)\n    ax2.set_title('Остатки L2 (Y - Y_hat)'); ax2.set_xlabel('Полочка'); ax2.set_ylabel('°C'); ax2.grid(True, axis='y', alpha=0.3); ax2.legend(ncol=min(4, deg_max))\n\n    # L_inf — точки и кривые\n    ax3.errorbar(x, y,\n                 xerr=g.get('x_std', pd.Series(index=g.index, dtype=float)),\n                 yerr=g.get('y_std', pd.Series(index=g.index, dtype=float)),\n                 fmt='o', capsize=3, label='точки')\n    pI_1 = next((f['poly'] for f in model_bank if f['method']=='L_inf' and f['deg']==1), None)\n    pI_N = next((f['poly'] for f in model_bank if f['method']=='L_inf' and f['deg']==deg_max), None)\n    if pI_1 is not None: ax3.plot(xs, pI_1(xs), label='L_inf deg=1')\n    if pI_N is not None and (pI_1 is None or pI_N.order != 1): ax3.plot(xs, pI_N(xs), label=f'L_inf deg={pI_N.order}')\n    ax3.set_title(f'{sensor}: аппроксимация L_inf' + ('' if _HAS_SCIPY else ' (IRLS-приближение)'))\n    ax3.set_xlabel(f'{sensor} (X)'); ax3.set_ylabel(f'T{REF_IDX} (Y)'); ax3.grid(True, alpha=0.3); ax3.legend()\n\n    # L_inf — остатки\n    idx = np.arange(n); width = 0.8 / max(deg_max,1)\n    for d in range(1, deg_max+1):\n        r = next(f['resid'] for f in model_bank if f['method']=='L_inf' and f['deg']==d)\n        ax4.bar(idx + (d-1 - (deg_max-1)/2)*width, r, width, label=f'deg={d}')\n    ax4.axhline(0, color='k', lw=0.8)\n    ax4.set_xticks(idx); ax4.set_xticklabels(labels)\n    ax4.set_title('Остатки L_inf (Y - Y_hat)'); ax4.set_xlabel('Полочка'); ax4.set_ylabel('°C'); ax4.grid(True, axis='y', alpha=0.3); ax4.legend(ncol=min(4, deg_max))\n\n    plt.tight_layout(); plt.show()\n\n# Соберём общую таблицу моделей и сохраним\ncalibration_models = pd.DataFrame(rows)\nout_csv = 'calibration_models.csv'\ncalibration_models.to_csv(out_csv, index=False)\nprint(f\"Сохранено: {out_csv} ({len(calibration_models)} строк)\")\n", "outputs": [], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": "# === 13) HTML-отчёт калибровки ===\nimport os, io, base64, datetime as _dt\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\nassert 'calibration_points_by_sensor' in globals() and not calibration_points_by_sensor.empty, 'Нет калибровочной таблицы.'\n\nREF_COL = f'T{REF_IDX}'\nREF_DISPLAY = globals().get('REF_NAME', REF_COL)\n\n# Формируем отображение имён: приоритет SENSOR_NAMES, затем FOLLOWER_NAMES по порядку T{REF_IDX+1}..T{REF_IDX+N_FOLLOW}\n_name_map = dict(globals().get('SENSOR_NAMES', {}))\n_follower_names = list(globals().get('FOLLOWER_NAMES', []))\n_ordered_cols = [f'T{REF_IDX+i}' for i in range(1, N_FOLLOW+1)]\nfor i, col in enumerate(_ordered_cols):\n    if i < len(_follower_names) and col not in _name_map:\n        _name_map[col] = _follower_names[i]\nNAME_MAP = _name_map\n\ndef _fig_to_b64(fig):\n    bio = io.BytesIO(); fig.savefig(bio, format='png', dpi=140, bbox_inches='tight'); plt.close(fig)\n    return base64.b64encode(bio.getvalue()).decode('ascii')\n\ndef _html_escape(s):\n    return (str(s).replace('&','&amp;').replace('<','&lt;').replace('>','&gt;'))\n\n# Определяем порядок датчиков по конфигурации (а не по алфавиту)\npresent = list(calibration_points_by_sensor['sensor'].unique())\nFOLLOW_ORDER = [c for c in _ordered_cols if c in present]\n\nnow = _dt.datetime.now().strftime('%Y-%m-%d %H:%M')\nparts = []\nparts.append(f'<h1>Краткий отчёт калибровки</h1>')\nparts.append(f'<p>Время формирования: {now}</p>')\nparts.append('<h2>Сенсоры</h2>')\nparts.append('<ul>')\nparts.append(f'<li>Эталон: {_html_escape(REF_DISPLAY)} ({_html_escape(REF_COL)})</li>')\nfor s in FOLLOW_ORDER:\n    parts.append(f\"<li>Калибруемый: {_html_escape(NAME_MAP.get(s, s))} ({_html_escape(s)})</li>\")\nparts.append('</ul>')\n\n# Калибровочные таблицы X–Y\nparts.append('<h2>Калибровочные точки X–Y</h2>')\nfor s in FOLLOW_ORDER:\n    g = calibration_points_by_sensor[calibration_points_by_sensor['sensor']==s].copy()\n    disp = NAME_MAP.get(s, s)\n    cols = ['bin_center','x_mean','y_mean','x_std','y_std','n_samples','start_date','end_date']\n    for c in cols:\n        if c not in g.columns:\n            g[c] = np.nan\n    g_disp = g[cols].copy()\n    parts.append(f'<h3>{_html_escape(disp)}</h3>')\n    parts.append(g_disp.to_html(index=False, float_format=lambda v: f\"{v:.6g}\"))\n\n# Результаты полиномиальной калибровки: сначала формулы степени (N-2), затем таблица\nif 'calibration_models' in globals() and calibration_models is not None and not calibration_models.empty:\n    parts.append('<h2>Модели калибровки и ошибки</h2>')\n    for s in FOLLOW_ORDER:\n        disp = NAME_MAP.get(s, s)\n        cm = calibration_models[calibration_models['sensor']==s]\n        if cm.empty:\n            continue\n        # Определяем максимальную степень N-2 (для данного сенсора)\n        n_pts = int(cm['n_points'].max()) if 'n_points' in cm.columns else len(calibration_points_by_sensor[calibration_points_by_sensor['sensor']==s])\n        deg_star = max(1, n_pts - 2)\n        best_L2_star = cm[(cm['method']=='L2') & (cm['degree']==deg_star)].head(1)\n        best_LI_star = cm[(cm['method']=='L_inf') & (cm['degree']==deg_star)].head(1)\n        parts.append(f'<h3>{_html_escape(disp)} — формулы (степень {deg_star})</h3>')\n        if not best_L2_star.empty:\n            r = best_L2_star.iloc[0]\n            parts.append(f\"<p><b>L2:</b> {_html_escape(r['formula'])} | RMSE={r['rmse']:.6g}, MAE={r['mae']:.6g}, Max|err|={r['maxerr']:.6g}</p>\")\n        if not best_LI_star.empty:\n            r = best_LI_star.iloc[0]\n            parts.append(f\"<p><b>L_inf:</b> {_html_escape(r['formula'])} | RMSE={r['rmse']:.6g}, MAE={r['mae']:.6g}, Max|err|={r['maxerr']:.6g}</p>\")\n        # Сводка качества по точкам\n        gp = calibration_points_by_sensor[calibration_points_by_sensor['sensor']==s]\n        if not gp.empty:\n            xstd_mean = float(np.nanmean(gp['x_std'])) if 'x_std' in gp else float('nan')\n            ystd_mean = float(np.nanmean(gp['y_std'])) if 'y_std' in gp else float('nan')\n            x_rng = (float(np.nanmin(gp['x_mean'])), float(np.nanmax(gp['x_mean'])))\n            y_rng = (float(np.nanmin(gp['y_mean'])), float(np.nanmax(gp['y_mean'])))\n            parts.append(f\"<p><i>Сводка:</i> N={len(gp)}, ⟨std X⟩≈{xstd_mean:.6g} °C, ⟨std Y⟩≈{ystd_mean:.6g} °C, диапазоны X=[{x_rng[0]:.6g}; {x_rng[1]:.6g}], Y=[{y_rng[0]:.6g}; {y_rng[1]:.6g}]</p>\")\n        # Полная таблица коэффициентов по всем степеням\n        show_cols = ['method','degree','rmse','mae','maxerr'] + sorted([c for c in cm.columns if c.startswith('c_p')], key=lambda x: int(x.split('p')[-1]), reverse=True)\n        parts.append(cm[show_cols].sort_values(['method','degree']).to_html(index=False, float_format=lambda v: f\"{v:.6g}\"))\nelse:\n    parts.append('<h2>Модели калибровки</h2><p>Таблица моделей недоступна. Сначала выполните ячейку 12.</p>')\n\n# Графики стабильных интервалов и их размеров — после формул\nif 'STABLE_BY_SENSOR' in globals() and STABLE_BY_SENSOR is not None and not STABLE_BY_SENSOR.empty:\n    parts.append('<h2>Стабильные интервалы (выбранные) и их размеры</h2>')\n    for s in FOLLOW_ORDER:\n        disp = NAME_MAP.get(s, s)\n        t = STABLE_BY_SENSOR[STABLE_BY_SENSOR['sensor']==s]\n        if t.empty:\n            continue\n        td = t.copy()\n        td['start_date'] = pd.to_datetime(td['start_date'])\n        td['end_date'] = pd.to_datetime(td['end_date'])\n        tdd = td.reset_index(drop=True)\n        fig, ax = plt.subplots(figsize=(8, max(2, 0.3*len(tdd))))\n        y = np.arange(len(tdd))\n        for i, row in tdd.iterrows():\n            sdt = row['start_date']; edt = row['end_date']\n            ax.hlines(i, sdt, edt, colors='C0', linewidth=6)\n            ax.text(edt, i, f\"  n={int(row['length'])}\", va='center', fontsize=8)\n        ax.set_yticks(y)\n        ax.set_yticklabels([f\"seg {i+1}\" for i in y])\n        ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d %H:%M'))\n        ax.set_title(f'{disp}: стабильные отрезки (выбранные)')\n        ax.set_xlabel('Время'); ax.set_ylabel('Сегменты')\n        ax.grid(True, axis='x', alpha=0.3)\n        fig.autofmt_xdate()\n        img_b64 = _fig_to_b64(fig)\n        parts.append(f'<p><img src=\"data:image/png;base64,{img_b64}\" alt=\"{_html_escape(disp)} intervals\"/></p>')\n\n# Детали аппроксимации — без неопределённости формулировки\nparts.append('<h2>Детали аппроксимации</h2>')\n_use_scipy = False\ntry:\n    import scipy\n    _use_scipy = True\nexcept Exception:\n    _use_scipy = False\nparts.append('<ul>')\nparts.append('<li>L2 — взвешенная МНК (веса 1/max(y_std, ε) при наличии std эталона).</li>')\nif _use_scipy:\n    parts.append('<li>L_inf — минимакс через линейное программирование (SciPy HiGHS).</li>')\nelse:\n    parts.append('<li>L_inf — минимакс, реализованный устойчивым IRLS‑методом.</li>')\nparts.append('<li>Степени перебираются от 1 до N−2 (где N — число калибровочных точек).</li>')\nparts.append('<li>Калибровочные точки формируются из самых длинных стабильных интервалов в центрированных корзинах 1°C.</li>')\nparts.append('</ul>')\n\nhtml = (\n    '<html><head><meta charset=\"utf-8\"><title>Отчёт калибровки</title>'\n    '<style>body{font-family:Segoe UI,Arial,sans-serif;line-height:1.35} '\n    'table{border-collapse:collapse} td,th{border:1px solid #ddd;padding:4px 6px} '\n    'h1,h2{margin-top:1em}</style>'\n    '</head><body>'\n    + '\n'.join(parts) +\n    '</body></html>'\n)\n\nout_path = os.path.join(os.getcwd(), 'calibration_report.html')\nwith open(out_path, 'w', encoding='utf-8') as f:\n    f.write(html)\nprint('HTML-отчёт сохранён:', out_path)\n", "outputs": [], "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": ""}}, "nbformat": 4, "nbformat_minor": 5}