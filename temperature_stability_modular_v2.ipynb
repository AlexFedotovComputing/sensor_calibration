{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Температурный пайплайн (модульный)\n*Последнее обновление: 2025-09-07*\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# === 1) Импорт и конфигурация ===\nimport os, io, csv, fnmatch, glob\nfrom typing import List, Tuple, Optional, Dict\n\nimport numpy as np\nimport pandas as pd\n\ntry:\n    from google.colab import files, drive  # type: ignore\n    IN_COLAB = True\nexcept Exception:\n    IN_COLAB = False\n\nfrom IPython.display import display\n\n# --- Конфигурация (меняйте по необходимости) ---\nDATE_FORMAT = ''          # '%Y-%m-%d %H:%M:%S' или пусто для авто\nREF_IDX = 8               # Эталон T{REF_IDX}\nN_FOLLOW = 3              # Кол-во следующих каналов\nWINDOW_N = 50             # Длина окна (сек)\nSTD_THR = 1e-3            # Порог std\nMIN_LEN = 50              # Мин. длина интервала\nGROUP_BY_FILE = True      # Индексы внутри файла\nTRIM_COLUMNS = True       # Обрезать лишние T-колонки\nFILE_PATTERNS = ['*.csv', '*.txt']\nSAMPLE_K = 1              # Сэмплирование при формировании пар\nINTERVAL_MODE = 'by_sensor'  # 'by_sensor' | 'joint'\n\n# Глобальные\nDATA = None\nSTABLE_JOINT = None\nSTABLE_BY_SENSOR = None\nPAIRS_DF = None\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# === 2) Загрузка/парсинг ===\ndef sniff_sep(sample: bytes) -> str:\n    try:\n        dialect = csv.Sniffer().sniff(sample.decode('utf-8', errors='ignore'),\n                                      delimiters=[',',';','\\t','|'])\n        return dialect.delimiter\n    except Exception:\n        line = sample.decode('utf-8', errors='ignore').splitlines()[0] if sample else ''\n        for cand in [',',';','\\t','|']:\n            if line.count(cand) >= 1:\n                return cand\n        return ','\n\ndef read_one_table(name: str, stream: io.BytesIO, date_format: Optional[str]=None) -> pd.DataFrame:\n    head = stream.read(8192); stream.seek(0)\n    sep = sniff_sep(head)\n    df = pd.read_csv(stream, sep=sep, engine='python')\n    if df.shape[1] < 17:\n        raise ValueError(f\"{name}: найдено {df.shape[1]} столбцов, требуется >= 17 (1 дата + 16 температур).\")\n    df = df.iloc[:, :17].copy()\n    df.columns = ['date'] + [f'T{i}' for i in range(16)]\n    if date_format and date_format.strip():\n        df['date'] = pd.to_datetime(df['date'], format=date_format, errors='coerce')\n    else:\n        df['date'] = pd.to_datetime(df['date'], infer_datetime_format=True, errors='coerce', dayfirst=True)\n    if df['date'].isna().any():\n        bad = int(df['date'].isna().sum()); print(f\"[Предупреждение] {name}: {bad} строк с нераспознанной датой отброшены.\")\n        df = df.dropna(subset=['date'])\n    for c in [f'T{i}' for i in range(16)]:\n        df[c] = pd.to_numeric(df[c], errors='coerce')\n    df['source_file'] = name\n    return df\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# === 3) Поиск стабильности ===\nfrom typing import Tuple\n\ndef rolling_std_mask(series: pd.Series, window: int, threshold: float) -> pd.Series:\n    rs = series.rolling(window=window, min_periods=window).std()\n    return (rs <= threshold)\n\ndef _segments_from_stable_mask(stable_mask: pd.Series, window: int) -> List[Tuple[int,int]]:\n    segs = []; cur = None\n    arr = stable_mask.to_numpy()\n    for i, ok in enumerate(arr):\n        if ok:\n            s = max(0, i - window + 1); e = i\n            if cur is None: cur = (s,e)\n            else:\n                cs, ce = cur\n                if s <= ce + 1: cur = (cs, max(ce, e))\n                else: segs.append(cur); cur = (s,e)\n        else:\n            if cur is not None: segs.append(cur); cur = None\n    if cur is not None: segs.append(cur)\n    return segs\n\ndef summarize_interval(df: pd.DataFrame, cols: List[str], s: int, e: int) -> Dict:\n    row = {'start_idx': int(s), 'end_idx': int(e), 'length': int(e - s + 1)}\n    row['start_date'] = pd.to_datetime(df.loc[s, 'date']) if s < len(df) else pd.NaT\n    row['end_date']   = pd.to_datetime(df.loc[e, 'date']) if e < len(df) else pd.NaT\n    for c in cols:\n        vals = df[c].to_numpy()[s:e+1]; good = ~np.isnan(vals)\n        row[f'mean_{c}'] = float(np.nanmean(vals)) if good.any() else np.nan\n        row[f'std_{c}']  = float(np.nanstd(vals, ddof=1)) if good.sum()>1 else np.nan\n    return row\n\ndef detect_stability(data: pd.DataFrame, ref_idx: int, follow_idxs: List[int],\n                     window: int, threshold: float, min_len: int, group_by_file: bool=True) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    ref_col = f'T{ref_idx}'; follow_cols = [f'T{i}' for i in follow_idxs]\n    for c in [ref_col]+follow_cols:\n        if c not in data.columns: raise ValueError(f\"Нет колонки: {c}\")\n    groups = list(data.groupby('source_file', sort=False)) if group_by_file and 'source_file' in data.columns else [('ALL', data)]\n    joint_rows, by_sensor_rows = [], []\n    for src, g in groups:\n        g = g.reset_index(drop=True)\n        mask_ref = rolling_std_mask(g[ref_col], window=window, threshold=threshold)\n        masks_others = {c: rolling_std_mask(g[c], window=window, threshold=threshold) for c in follow_cols}\n        joint_mask = mask_ref.copy()\n        for c in follow_cols: joint_mask &= masks_others[c]\n        joint_segs = [(s,e) for (s,e) in _segments_from_stable_mask(joint_mask, window) if (e-s+1)>=int(min_len)]\n        for (s,e) in joint_segs:\n            row = {'source_file': src, 'ref': ref_col, 'followers': ','.join(follow_cols)}\n            row.update(summarize_interval(g, [ref_col]+follow_cols, s, e))\n            if src!='ALL':\n                first_idx = int(data.index[data['source_file']==src][0])\n                row['start_idx_abs'] = row['start_idx']+first_idx; row['end_idx_abs']=row['end_idx']+first_idx\n            else:\n                row['start_idx_abs']=row['start_idx']; row['end_idx_abs']=row['end_idx']\n            joint_rows.append(row)\n        for c in follow_cols:\n            gated = masks_others[c] & mask_ref\n            segs = [(s,e) for (s,e) in _segments_from_stable_mask(gated, window) if (e-s+1)>=int(min_len)]\n            for (s,e) in segs:\n                row = {'source_file': src, 'sensor': c, 'ref': ref_col}\n                row.update(summarize_interval(g, [c, ref_col], s, e))\n                if src!='ALL':\n                    first_idx = int(data.index[data['source_file']==src][0])\n                    row['start_idx_abs'] = row['start_idx']+first_idx; row['end_idx_abs']=row['end_idx']+first_idx\n                else:\n                    row['start_idx_abs']=row['start_idx']; row['end_idx_abs']=row['end_idx']\n                by_sensor_rows.append(row)\n    joint = pd.DataFrame(joint_rows).sort_values(['source_file','start_idx']) if joint_rows else pd.DataFrame()\n    by_sensor = pd.DataFrame(by_sensor_rows).sort_values(['source_file','sensor','start_idx']) if by_sensor_rows else pd.DataFrame()\n    return joint, by_sensor\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# === 4) Вариант A: загрузка через диалог (Colab) ===\nif not IN_COLAB:\n    print('Диалог загрузки доступен только в Google Colab.')\nelse:\n    print('Выберите один или несколько .csv/.txt файлов…')\n    uploads = files.upload()\n    SELECTED_FILES = [('uploaded:' + name, content) for name, content in uploads.items()]\n    IS_BYTES_INPUT = True\n    print('Загружено файлов:', len(SELECTED_FILES))\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# === 5) Вариант B: выбор папки в Google Drive (Colab) ===\nif not IN_COLAB:\n    print('Доступ к Google Drive возможен только в Colab.')\nelse:\n    drive.mount('/content/drive')\n    ROOT_DIR = '/content/drive/MyDrive'  # при необходимости поменяйте\n    found = []\n    for pat in FILE_PATTERNS:\n        found.extend(glob.glob(os.path.join(ROOT_DIR, '**', pat), recursive=True))\n    found = sorted(found)\n    print(f'Найдено файлов: {len(found)}')\n    SELECTED_FILES = found\n    IS_BYTES_INPUT = False\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# === 6) Обработка и объединение данных ===\nassert 'SELECTED_FILES' in globals(), 'Сначала выполните ячейку 4 или 5.'\n\nframes, errors = [], []\nfor item in SELECTED_FILES:\n    try:\n        if IS_BYTES_INPUT:\n            name, content = item\n            bio = io.BytesIO(content)\n            df = read_one_table(name, bio, date_format=DATE_FORMAT or None)\n        else:\n            path = item\n            with open(path, 'rb') as f:\n                df = read_one_table(os.path.basename(path), io.BytesIO(f.read()), date_format=DATE_FORMAT or None)\n        frames.append(df)\n    except Exception as e:\n        errors.append((str(item), str(e)))\n\nif not frames:\n    raise RuntimeError('Не удалось прочитать ни один файл.')\n\ndata = pd.concat(frames, ignore_index=True).sort_values('date').reset_index(drop=True)\n\nfollow_idxs = [REF_IDX + i for i in range(1, N_FOLLOW + 1) if REF_IDX + i <= 15]\ncols_keep = ['date', f'T{REF_IDX}'] + [f'T{i}' for i in follow_idxs] + ['source_file']\nif TRIM_COLUMNS:\n    data = data[[c for c in cols_keep if c in data.columns]]\n\nDATA = data\nprint('Итоговые размеры набора:', data.shape)\nprint(f'Эталон: T{REF_IDX}; следующие:', [f\"T{i}\" for i in follow_idxs])\ndisplay(data.head(10))\n\nif errors:\n    print('\\\\nОшибки чтения (первые 10):')\n    for p, msg in errors[:10]:\n        print(' -', p, ':', msg)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# === 7) Поиск стабильных интервалов ===\nassert DATA is not None, 'Нет данных DATA.'\nfollow_idxs = [REF_IDX + i for i in range(1, N_FOLLOW + 1) if REF_IDX + i <= 15]\n\nSTABLE_JOINT, STABLE_BY_SENSOR = detect_stability(\n    DATA, ref_idx=REF_IDX, follow_idxs=follow_idxs,\n    window=WINDOW_N, threshold=STD_THR, min_len=MIN_LEN, group_by_file=GROUP_BY_FILE\n)\n\nprint('Совместные интервалы:', 0 if STABLE_JOINT is None else len(STABLE_JOINT))\nprint('По каналам:', 0 if STABLE_BY_SENSOR is None else len(STABLE_BY_SENSOR))\nif STABLE_JOINT is not None and not STABLE_JOINT.empty:\n    display(STABLE_JOINT.head(10))\nif STABLE_BY_SENSOR is not None and not STABLE_BY_SENSOR.empty:\n    display(STABLE_BY_SENSOR.head(10))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7.1 Отбор плато: самое длинное в пределах ±DEG_TOL °C по эталону\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Параметры отбора\nDEG_TOL = globals().get('DEG_TOL', 1.0)                  # допуск по эталону (°C)\nSELECT_LONGEST_PER_DEGREE = globals().get('SELECT_LONGEST_PER_DEGREE', True)\nCLUSTER_STRATEGY = globals().get('CLUSTER_STRATEGY', 'sliding')  # 'sliding' | 'bucket'\n\ndef _interval_ref_level(df, ref_col, s, e):\n    vals = df[ref_col].to_numpy()[int(s):int(e)+1]\n    return float(np.nanmedian(vals))\n\ndef _filter_longest_per_degree(table: pd.DataFrame, data: pd.DataFrame, ref_idx: int, mode: str,\n                               deg_tol: float = 1.0, strategy: str = 'sliding') -> pd.DataFrame:\n    if table is None or table.empty:\n        return table\n    ref_col = f\"T{ref_idx}\"\n    tbl = table.copy()\n    ref_levels = []\n    for _, row in tbl.iterrows():\n        src = row['source_file']; s, e = int(row['start_idx']), int(row['end_idx'])\n        g = data[data['source_file'] == src].reset_index(drop=True) if src != 'ALL' else data\n        ref_levels.append(_interval_ref_level(g, ref_col, s, e))\n    tbl['ref_level'] = ref_levels\n\n    group_keys = ['source_file','sensor'] if (mode=='by_sensor' and 'sensor' in tbl.columns) else ['source_file']\n    out = []\n    for _, grp in tbl.sort_values('ref_level').groupby(group_keys, as_index=False):\n        g = grp.sort_values('ref_level').reset_index(drop=True)\n        if strategy == 'bucket':\n            bins = np.floor(g['ref_level'] / deg_tol).astype(int)\n            g = g.assign(_bin=bins)\n            keep = g.sort_values('length', ascending=False).groupby('_bin', as_index=False).head(1)\n            out.append(keep.drop(columns=['_bin']))\n        else:\n            i, n = 0, len(g)\n            while i < n:\n                start_val = g.loc[i, 'ref_level']; j = i\n                while j+1 < n and (g.loc[j+1, 'ref_level'] - start_val) <= deg_tol:\n                    j += 1\n                cluster = g.loc[i:j].copy()\n                keep = cluster.sort_values(['length','end_idx'], ascending=[False, False]).iloc[0:1]\n                out.append(keep); i = j + 1\n    return pd.concat(out, ignore_index=True) if out else tbl\n\n# Применяем к STABLE_* (после шага 7)\ntry:\n    follow_idxs = [REF_IDX + i for i in range(1, N_FOLLOW + 1) if REF_IDX + i <= 15]\n    if SELECT_LONGEST_PER_DEGREE and DATA is not None:\n        if 'STABLE_BY_SENSOR' in globals() and STABLE_BY_SENSOR is not None and not STABLE_BY_SENSOR.empty:\n            STABLE_BY_SENSOR = _filter_longest_per_degree(STABLE_BY_SENSOR, DATA, REF_IDX, mode='by_sensor',\n                                                          deg_tol=DEG_TOL, strategy=CLUSTER_STRATEGY)\n            print(f\"STABLE_BY_SENSOR после отбора → {len(STABLE_BY_SENSOR)} интервалов\")\n            display(STABLE_BY_SENSOR.head(10))\n        if 'STABLE_JOINT' in globals() and STABLE_JOINT is not None and not STABLE_JOINT.empty:\n            STABLE_JOINT = _filter_longest_per_degree(STABLE_JOINT, DATA, REF_IDX, mode='joint',\n                                                      deg_tol=DEG_TOL, strategy=CLUSTER_STRATEGY)\n            print(f\"STABLE_JOINT после отбора → {len(STABLE_JOINT)} интервалов\")\n            display(STABLE_JOINT.head(10))\nexcept NameError:\n    print(\"Сначала выполните шаг 7, затем 7.1.\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# === 8) Формирование пар (X=датчик, Y=эталон) ===\ndef build_pairs(data: pd.DataFrame,\n                stable_joint: Optional[pd.DataFrame],\n                stable_by_sensor: Optional[pd.DataFrame],\n                ref_idx: int,\n                follow_idxs: List[int],\n                mode: str = 'by_sensor',\n                sample_k: int = 1) -> pd.DataFrame:\n    ref_col = f'T{ref_idx}'\n    pairs_rows = []\n    if mode == 'by_sensor':\n        assert stable_by_sensor is not None and not stable_by_sensor.empty, 'Нет STABLE_BY_SENSOR.'\n        table = stable_by_sensor\n        for src in table['source_file'].unique():\n            tsub = table[table['source_file'] == src]\n            g = data[data['source_file'] == src].reset_index(drop=True) if src != 'ALL' else data\n            for _, row in tsub.iterrows():\n                sensor = row['sensor']\n                s, e = int(row['start_idx']), int(row['end_idx'])\n                x = g[sensor].to_numpy()[s:e+1]; y = g[ref_col].to_numpy()[s:e+1]; dates = g['date'].to_numpy()[s:e+1]\n                mask = (~np.isnan(x)) & (~np.isnan(y))\n                x, y, dates = x[mask], y[mask], dates[mask]\n                if sample_k > 1 and len(x) > 0:\n                    x, y, dates = x[::sample_k], y[::sample_k], dates[::sample_k]\n                for xi, yi, ti in zip(x, y, dates):\n                    pairs_rows.append({'source_file': src, 'sensor': sensor, 'ref': ref_col,\n                                       'x_sensor': float(xi), 'y_ref': float(yi), 'date': pd.to_datetime(ti)})\n    else:\n        assert stable_joint is not None and not stable_joint.empty, 'Нет STABLE_JOINT.'\n        table = stable_joint\n        follow_cols = [f'T{i}' for i in follow_idxs]\n        for src in table['source_file'].unique():\n            tsub = table[table['source_file'] == src]\n            g = data[data['source_file'] == src].reset_index(drop=True) if src != 'ALL' else data\n            for _, row in tsub.iterrows():\n                s, e = int(row['start_idx']), int(row['end_idx'])\n                for sensor in follow_cols:\n                    x = g[sensor].to_numpy()[s:e+1]; y = g[ref_col].to_numpy()[s:e+1]; dates = g['date'].to_numpy()[s:e+1]\n                    mask = (~np.isnan(x)) & (~np.isnan(y))\n                    x, y, dates = x[mask], y[mask], dates[mask]\n                    if sample_k > 1 and len(x) > 0:\n                        x, y, dates = x[::sample_k], y[::sample_k], dates[::sample_k]\n                    for xi, yi, ti in zip(x, y, dates):\n                        pairs_rows.append({'source_file': src, 'sensor': sensor, 'ref': ref_col,\n                                           'x_sensor': float(xi), 'y_ref': float(yi), 'date': pd.to_datetime(ti)})\n    return pd.DataFrame(pairs_rows)\n\nassert DATA is not None, 'Нет данных DATA.'\nfollow_idxs = [REF_IDX + i for i in range(1, N_FOLLOW + 1) if REF_IDX + i <= 15]\nPAIRS_DF = build_pairs(DATA, STABLE_JOINT, STABLE_BY_SENSOR, REF_IDX, follow_idxs,\n                       mode=INTERVAL_MODE, sample_k=SAMPLE_K)\n\nprint('Сформировано пар:', 0 if PAIRS_DF is None else len(PAIRS_DF))\nif PAIRS_DF is not None and not PAIRS_DF.empty:\n    for sensor in list(PAIRS_DF['sensor'].unique())[:2]:\n        print(f'\\n{sensor} (пример):')\n        display(PAIRS_DF[PAIRS_DF['sensor']==sensor].head(8))\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# === 9) Сохранение результатов ===\nassert DATA is not None, 'Нет DATA.'\nout_dir = '/content' if IN_COLAB else os.getcwd()\n\nDATA.to_csv(os.path.join(out_dir, 'combined_temperatures.csv'), index=False)\nif STABLE_JOINT is not None and not STABLE_JOINT.empty:\n    STABLE_JOINT.to_csv(os.path.join(out_dir, 'stable_intervals_joint.csv'), index=False)\nif STABLE_BY_SENSOR is not None and not STABLE_BY_SENSOR.empty:\n    STABLE_BY_SENSOR.to_csv(os.path.join(out_dir, 'stable_intervals_by_sensor.csv'), index=False)\nif PAIRS_DF is not None and not PAIRS_DF.empty:\n    PAIRS_DF.to_csv(os.path.join(out_dir, 'ref_pairs_by_sensor.csv'), index=False)\n    cnt = 0\n    for sensor, g in PAIRS_DF.groupby('sensor'):\n        g.to_csv(os.path.join(out_dir, f'ref_pairs_{sensor}.csv'), index=False)\n        cnt += 1\n    print(f'Сохранено файлов пар по датчикам: {cnt}')\nprint('Файлы записаны в:', out_dir)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# === 10) (опционально) Быстрый график ===\ntry:\n    import matplotlib.pyplot as plt\n    if PAIRS_DF is not None and not PAIRS_DF.empty:\n        one = PAIRS_DF[PAIRS_DF['sensor'] == PAIRS_DF['sensor'].unique()[0]].head(1000)\n        plt.figure()\n        plt.scatter(one['x_sensor'], one['y_ref'], s=10)\n        plt.xlabel('Sensor (X)'); plt.ylabel('Reference (Y)')\n        plt.title(f\"Scatter: {one['sensor'].iloc[0]} vs ref\")\n        plt.show()\n    else:\n        print('Нет данных PAIRS_DF для графика.')\nexcept Exception as e:\n    print('График пропущен:', e)\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}